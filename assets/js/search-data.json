{
  
    
        "post0": {
            "title": "Fastai Image Augmentation",
            "content": "Intro . Fastai2 provides a wide range of data augmentation techniques and this blog is particularly focused on image augmentation techniques (This is a update to the article &#39;Data Augmentation Techniques&#39; I wrote in 2018 using fastai v1[1]) . Working with limited data has its own challenges, using data augmentation can have positive results only if the augmentation techniques enhance the current data set for example is there any worth is training a network to ‘learn’ about a landmark in a flipped upside down orientation? . Invariance is the ability of convolutional neural networks to classify objects even when they are placed in different orientations. Data augmentation is a way of creating new ‘data’ with different orientations. The benefits of this are two fold, the first being the ability to generate ‘more data’ from limited data and secondly it prevents over fitting. . Most deep learning libraries use a step by step method of augmentation whilst *fastai2 utilizes methods that combine various augmentation parameters to reduce the number of computations and reduce the number of lossy operations*[2]. . Fastai uses Pipelines to compose several transforms together. A Pipeline is defined by passing a list of Transforms and it will then compose the transforms inside it. In this blog I will look at what order these transforms are conducted and what effect they have on image quality and efficiency. Pipelines are sorted by the internal order atribute (more discussed below) with a default order of 0. . Using this as a high-level API example . from fastai2.vision.all import* source = untar_data(URLs.PETS) . #High-level API example testblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=[Resize(256)], batch_tfms=[*aug_transforms(xtra_tfms=None)]) test_dls = testblock.dataloaders(source/&#39;images&#39;) . test_dls.show_batch(max_n=6, nrows=1, ncols=6) . To check the order of how augementations are conducted we can call *after_item* and *after_batch* . after_item . test_dls.after_item . Pipeline: Resize -&gt; ToTensor . In this case images are: . resized to sizes of equal length, in this case 256 and then . convert the image into a *channel* X *height* X *weigth* tensor . But what does Resize do? . Click the button to view the Resize class . #collapse #https://github.com/fastai/fastai2/blob/master/nbs/09_vision.augment.ipynb @delegates() class Resize(RandTransform): split_idx = None mode,mode_mask,order,final_size = Image.BILINEAR,Image.NEAREST,1,None &quot;Resize image to `size` using `method`&quot; def __init__(self, size, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection, resamples=(Image.BILINEAR, Image.NEAREST), **kwargs): super().__init__(**kwargs) self.size,self.pad_mode,self.method = _process_sz(size),pad_mode,method self.mode,self.mode_mask = resamples def before_call(self, b, split_idx): if self.method==ResizeMethod.Squish: return self.pcts = (0.5,0.5) if split_idx else (random.random(),random.random()) def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)): orig_sz = _get_sz(x) self.final_size = self.size if self.method==ResizeMethod.Squish: return x.crop_pad(orig_sz, Tuple(0,0), orig_sz=orig_sz, pad_mode=self.pad_mode, resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size) w,h = orig_sz op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad] m = w/self.size[0] if op(w/self.size[0],h/self.size[1]) else h/self.size[1] cp_sz = (int(m*self.size[0]),int(m*self.size[1])) tl = Tuple(int(self.pcts[0]*(w-cp_sz[0])), int(self.pcts[1]*(h-cp_sz[1]))) return x.crop_pad(cp_sz, tl, orig_sz=orig_sz, pad_mode=self.pad_mode, resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size) . . By default resize &#39;squishes&#39; the image to the size specified. The image is resized so that the shorter dimension matches the size specifed and the rest padded with what is specified in pad_mode. . The method parameter can be be 1 of 3 values: Crop, Pad or Squish(default) eg: *method=ResizeMethod.Squish* The padding parameter also takes 1 of 3 values: Border, Zeros and Reflection(default) eg: *pad_mode=PadMode.Reflection*. . The images are resized/resamples using bilinear and nearest neighbour interprolations[3]. . We can check to see how initial image sizes are affected by Resize. I choose an image with numbers so that you can see different areas of the image easier and I colored each of the corners a different color to better see what effects Resize has on the image. . #Load a test image image_path = &#39;C:/Users/avird/.fastai/data/0100-number_12.jpg&#39; img = Image.open(image_path) img.shape, type(img) . ((380, 500), PIL.JpegImagePlugin.JpegImageFile) . #Convert image into a fastai.PILImage img = PILImage(PILImage.create(image_path).resize((500,380))) img.shape, type(img) . ((380, 500), fastai2.vision.core.PILImage) . #View the image img . Fastai uses 3 types of resize methods (using *ResizeMethod*: Squish, Pad and Crop) and they can be plotted against each other to view the differences between them. Squish is the fastai default. To better view the differences I used a padding of zeros.(the default for padding is Reflection) . &gt; Image size 5 . #collapse #Use image size of 5 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(5, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=1), ctx=ax, title=f&#39;{method}, size=5&#39;); . . Using an image size of 5 we can see how the image is affected by Resize. At this size we can see all the 4 different colors in each corner and there is not much difference between squish abd crop. With pad however the image is being resized so the shorter dimension (in this case the height (as the original image size is 380 height and 500 width) is matched to the image size of 256 and then padded with zeros. . &gt; Image size 15 . #collapse #Use image size of 15 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(15, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=1), ctx=ax, title=f&#39;{method}, size=15&#39;); . . At image size 15, both &#39;squish&#39; and &#39;pad&#39; are still showing all the colors in the corners but with &#39;crop&#39; you start to notice that the colors in each corner are begining to fade as the image is being cropped from the center . &gt; Image size 256 . #collapse #Use image size of 256 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(256, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img, split_idx=0), ctx=ax, title=f&#39;{method}, size=256&#39;); . . At 256 both &#39;squish&#39; and &#39;pad&#39; still display the full image and &#39;crop&#39; displays the cropped image . What impacts could this have on real datasets. . Using an image from a Covid19 dataset [5] . #collapse test_path = &#39;C:/Users/avird/.fastai/data/0002.jpeg&#39; testimg = Image.open(test_path) img2 = PILImage(PILImage.create(test_path).resize((944, 656))) img2 . . #collapse #Use image size of 256 _,axs = plt.subplots(1,3,figsize=(20,20)) for ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]): rsz = Resize(256, method=method, pad_mode=PadMode.Zeros) show_image(rsz(img2, split_idx=1), ctx=ax, title=f&#39;{method}, size=256&#39;); . . In this case: . the default &#39;squish&#39; resize method squishes the image on the horizontal axis. You can view the whole image however you can see that ribcage has been constricted towards the center. The implications of this could mean that important features you see in the original image could either be erased or diluted. . for the &#39;pad&#39; resize the image is still viewable fully but again the image has been squished on the vertical axis. . With &#39;crop&#39;, the image is cropped from the centre hence we lose image details from the edges . The implications of these choices is really dependant on the dataset but they could have an detrimental effect if the wrong choice is choosen leading to vital features being erased or diluted . after_batch . Back to the pets example if we run *after_batch*, this shows us the after batch augmentation pipeline. Previously item_tfms is used to resize the images and to collate them into tensors ready for GPU processing. . test_dls.after_batch . Pipeline: IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm . This reveals the pipeline process for the batch transformations: . convert ints to float tensors . apply all the affine transformations . followed by the lighting transformations . The order is important in order to maintain a number of key aspects:&gt; Maintain image quality . Reduce computations . Improve efficiency . As mentioned in Fastbook [4], most machine libraries use a step by step process of augmentation which can lead to a reduced quality of images. The datablock example above is an example of a high-level API which is pretty flexible but not as much as a mid-level API. . The mid-level datablock below is an exact example of the high-level datablock above and allows for more customizations and we will use this datablock for the rest of the blog . #collapse #Helper for viewing single images def repeat_one(source, n=128): &quot;&quot;&quot;Single image helper for displaying batch&quot;&quot;&quot; return [get_image_files(source)[1]]*n . . Mid-Level API and viewing a batch of a single image . #mid-level API example #num_workers = 0 because I use windows :) and windows does not support multiprocessing on CUDA [6] tfms = [[PILImage.create], [using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), Categorize]] item_tfms = [ToTensor(), Resize(256)] splitter=RandomSplitter(seed=42) after_b = [IntToFloatTensor(), *aug_transforms(xtra_tfms=RandomResizedCrop(256), min_scale=0.9)] dsets = Datasets(repeat_one(source/&#39;images&#39;), tfms=tfms) dls = dsets.dataloaders(after_item=item_tfms, after_batch=after_b, bs=32, num_workers=0, splits=splitter) . dls.after_batch . Pipeline: RandomResizedCrop -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm . #collapse dls.show_batch(max_n=6, nrows=1, ncols=6) . . Image comparisons (Fastai v The Rest) . #create 1 batch x,y = dls.one_batch() . Checking image quality and speed using step by step transformations. . #collapse time x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=256) x1 = x1.brightness(max_lighting=0.2, p=1.) x1 = x1.zoom(max_zoom=1.1, p=0.5) x1 = x1.warp(magnitude=0.2, p=0.5) _,axs = subplots(1, 1, figsize=(5,5)) TensorImage(x1[0]).show(ctx=axs[0]) . . Wall time: 103 ms . &lt;matplotlib.axes._subplots.AxesSubplot at 0x29421ecdfc8&gt; . Checking image quality and speed using fastai2 . #collapse %%time tfms = setup_aug_tfms([Brightness(max_lighting=0.2, p=1.,), CropPad(size=256), Zoom(max_zoom=1.1, p=0.5), Warp(magnitude=0.2, p=0.5) ]) x = Pipeline(tfms)(x) _,axs = subplots(1, 1, figsize=(5,5)) TensorImage(x[0]).show(ctx=axs[0]) . . Wall time: 45.9 ms . &lt;matplotlib.axes._subplots.AxesSubplot at 0x294220787c8&gt; . Comparing the times above using a pipeline where a list of transforms are passed in is nearly twice as fast as using augmentations step by step. In this case the step by step method completed the task in 103ms compard to 46s using fastai . Looking at side by side look at image quality . #collapse image_comp(): x,y = dls.one_batch() tfms = setup_aug_tfms([Brightness(max_lighting=0.3, p=1.,), Resize(size=256), Zoom(max_zoom=1.1, p=1.), Warp(magnitude=0.2, p=1.) ]) x = Pipeline(tfms)(x) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=256) x1 = x1.brightness(max_lighting=0.3, p=1.) x1 = x1.zoom(max_zoom=1.1, p=1.) x1 = x1.warp(magnitude=0.2, p=1.) _,axs = subplots(1, 2, figsize=(20,20)) TensorImage(x[0]).show(ctx=axs[0], title=&#39;fastai&#39;) TensorImage(x1[0]).show(ctx=axs[1], title=&#39;other&#39;) . . image_comp() . You can definately see differences between the two pictures, the &#39;fastai&#39; image is more clearer compared to the &#39;other&#39; image. How about some other examples . image_comp() . image_comp() . List of Transforms . There a number of transforms and here is a list of the most common ones . RandomResizedCrop = &quot;Picks a random scaled crop of an image and resize it to size - order 1&quot; IntToFloatTensor = &quot;Transform image to float tensor, optionally dividing by 255 (e.g. for images) - order 10 Rotate = &quot;Apply a random rotation of at most max_deg with probability p to a batch of images&quot; Brightness = &quot;Apply change in brightness of max_lighting to batch of images with probability p.&quot; RandomErasing = &quot;Randomly selects a rectangle region in an image and randomizes its pixels.&quot; - order 100 CropPad = &quot;Center crop or pad an image to size&quot; - order 0 Zoom = &quot;Apply a random zoom of at most max_zoom with probability p to a batch of images&quot; Warp = &quot;Apply perspective warping with magnitude and p on a batch of matrices&quot; Contrast = &quot;Apply change in contrast of max_lighting to batch of images with probability p.&quot; . Pipeline for multiple augmentations . In the example above the after_batch pipeline consisted of IntToFloatTensor &gt; Affine tranformations &gt; Lighting transformations. . However what we uses additional augmentations, what does the pipeline look like then? . #collapse source = untar_data(URLs.PETS) #Helper for viewing single images def repeat_one(source, n=128): &quot;&quot;&quot;Single image helper for displaying batch&quot;&quot;&quot; return [get_image_files(source)[2]]*n . . #collapse #Include multiple transforms tfms = [[PILImage.create], [using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), Categorize]] item_tfms = [ToTensor(), Resize(296)] splitter=RandomSplitter(seed=42) xtra_tfms = [Rotate(max_deg=45, p=1.), RandomErasing(p=1., max_count=10, min_aspect=0.5, sl=0.2, sh=0.2), RandomResizedCrop(p=1., size=256), Brightness(max_lighting=0.2, p=1.), CropPad(size=256), Zoom(max_zoom=2.1, p=0.5), Warp(magnitude=0.2, p=1.0) ] after_b = [IntToFloatTensor(), *aug_transforms(mult=1.0, do_flip=False, flip_vert=False, max_rotate=0., max_zoom=1.1, max_lighting=0.,max_warp=0., p_affine=0.75, p_lighting=0.75, xtra_tfms=xtra_tfms, size=256, mode=&#39;bilinear&#39;, pad_mode=PadMode.Reflection, align_corners=True, batch=False, min_scale=0.9)] mdsets = Datasets(repeat_one(source/&#39;images&#39;), tfms=tfms) mdls = mdsets.dataloaders(after_item=item_tfms, after_batch=after_b, bs=32, num_workers=0, splits=splitter) . . Looking at after_item - it is the same as before . mdls.after_item . Pipeline: Resize -&gt; ToTensor . mdls.after_batch . Pipeline: CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm -&gt; RandomErasing . after_batch is now a different story and we can see the list of how fastai computes its augmentations. These are all done in sequence (depending on their order) starting with . CropPad . followed by affine . lighting . and random erasing transforms. . Here is what the batch looks like . mdls.show_batch(max_n=6, nrows=1, ncols=6) . The order number determines the sequence of the transforms for example CropPad is order 0, Resize and RandomCrop are order 1 hence the reason they appear first on the list. IntToFloatTensor is order 10 and runs after PIL transforms on the GPU. Affine transforms are order 30 and so is RandomResizedCropGPU and lighting transforms are order 40. RandomErasing is order 100. . Viewing the order of transforms . #for example mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . RandomResizedCrop.order, CropPad.order, IntToFloatTensor.order, AffineCoordTfm.order, RandomResizedCropGPU.order, RandomErasing.order . (0, 0, 10, 30, 30, 100) . You can force the order by implicity specifying the order of a transform by stating the order within a transform class. . Interesting Observations . There were some interesting observations during this experimention. Adding a *min_scale* value in aug_transforms adds RandomResizedCropGPU to the pipeline . mdls.after_batch . Pipeline: CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . However if you add RandomResizedCrop as well as a min_scale value the pipeline now looks like this . mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; RandomResizedCropGPU -&gt; LightingTfm -&gt; RandomErasing . And if you use RandomResizedCrop with no min_scale value the pipeline is now: . mdls.after_batch . Pipeline: RandomResizedCrop -&gt; CropPad -&gt; IntToFloatTensor -&gt; AffineCoordTfm -&gt; LightingTfm -&gt; RandomErasing . Still to do . There is clearly a plethora of options and additonal experimentation is needed to see what the impact of the various pipelines are on image quality, efficiency and end results -*work in progress* . Manually going through the pipeline . Attempt to manually go throught the pipeline. . #collapse image_path = &#39;C:/Users/avird/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39; TEST_IMAGE = Image.open(image_path) img = PILImage(PILImage.create(image_path)) img.shape, type(img) . . ((400, 600), fastai2.vision.core.PILImage) . This is the original image of size 400 height and 600 width . #collapse #Original Image img . . Resize to 256 using default crop and reflection padding . #collapse #Resize to 256 using default crop and reflection padding r = Resize(256, method=ResizeMethod.Crop, pad_mode=PadMode.Reflection) w = r(img) w.shape, type(w) . . ((256, 256), fastai2.vision.core.PILImage) . #collapse w . . Crop the image using size 256 . #collapse Crop crp = CropPad(256) c = r(crp(img)) c.shape, type(c) . . ((256, 256), fastai2.vision.core.PILImage) . #collapse c . . Convert PILImage into a TensorImage . timg = TensorImage(array(c)).permute(2,0,1).float()/255. timg.shape, type(timg) . (torch.Size([3, 256, 256]), fastai2.torch_core.TensorImage) . h = TensorImage(timg[None].expand(3, *timg.shape).clone()) h.shape, type(h) . (torch.Size([3, 3, 256, 256]), fastai2.torch_core.TensorImage) . #if do_flip=true and flip-vert=false = Flip fli = Flip(p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y = fli(h) show_image(y[0], ctx=ax, cmap=&#39;Greys&#39;) . #if do_flp=true and flip_vert=true = dihyderal dih = Dihedral(p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y1 = dih(h) show_image(y1[0], ctx=ax) . #Rotate rot = Rotate(max_deg=45, p=1.) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y2 = rot(h) show_image(y2[0], ctx=ax) . # Zoom zoo = Zoom(max_zoom=4.1, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y3 = zoo(h) show_image(y3[0], ctx=ax) . # Warp war = Warp(magnitude=0.7, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y4 = war(h) show_image(y4[0], ctx=ax) . #Brightness bri = h.brightness(draw=0.9, p=1.) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y5 = bri show_image(y5[0], ctx=ax) . #Contrast con = h.contrast(draw=1.9, p=0.5) _,axs = plt.subplots(1,2, figsize=(12,6)) for i,ax in enumerate(axs.flatten()): y6 = con show_image(y6[0], ctx=ax) . View the images side by side . #collapse _,axs = plt.subplots(1,8, figsize=(20,9)) for i,ax in enumerate(axs.flatten()): y7 = y1 + y4 show_image(img, ctx=axs[0], title=&#39;original&#39;) show_image(w, ctx=axs[1], title=&#39;resize 256&#39;) show_image(y[0], ctx=axs[2], title=&#39;flip&#39;) show_image(y2[0], ctx=axs[3], title=&#39;rotate&#39;) show_image(y3[0], ctx=axs[4], title=&#39;zoom&#39;) show_image(y4[0], ctx=axs[5], title=&#39;warp&#39;) show_image(y5[0], ctx=axs[6], title=&#39;brighness&#39;) show_image(y6[0], ctx=axs[7], title=&#39;contrast&#39;) . . References: . 1: https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b . 2: https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb . 3: https://github.com/fastai/fastai2/blob/master/nbs/09_vision.augment.ipyn . 4: https://github.com/fastai/fastbook . 5: https://github.com/lindawangg/COVID-Net . 6: https://forums.fast.ai/t/windows-runtimeerror-cuda-runtime-error-801-runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type/57333 .",
            "url": "https://asvcode.github.io/medical_imaging/fastai/augmentation/image-augmentation/2020/03/26/Fastai2-Image-Augmentation.html",
            "relUrl": "/fastai/augmentation/image-augmentation/2020/03/26/Fastai2-Image-Augmentation.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://asvcode.github.io/medical_imaging/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Test",
            "content": "What are Dicom images? . DICOM(Digital Imaging and COmmunications in Medicine) is the de-facto standard that establishes rules that allow medical images(X-Ray, MRI, CT) and associated information to be exchanged between imaging equipment from different vendors, computers, and hospitals. The DICOM format provides a suitable means that meets health infomation exchange (HIE) standards for transmision of health related data among facilites and HL7 standards which is the messaging standard that enables clinical applications to exchange data. . . DICOM files typically have a .dcm extension and provides a means of storing data in seperate &#39;tags&#39; such as patient information as well as image/pixel data. A DICOM file consists of a header and image data sets packed into a single file. The information within the header is organized as a constant and standardized series of tags. By extracting data from these tags one can access important information regarding the patient demographics, study parameters, etc . 16 bit DICOM images have values ranging from -32768 to 32768 while 8-bit greyscale images store values from 0 to 255. The value ranges in DICOM images are useful as they correlate with the Hounsfield Scale which is a quantitative scale for describing radiodensity . Parts of a DICOM .",
            "url": "https://asvcode.github.io/medical_imaging/test/2020/01/01/Test.html",
            "relUrl": "/test/2020/01/01/Test.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://asvcode.github.io/medical_imaging/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://asvcode.github.io/medical_imaging/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}